---
title: "CVTDM FINAL PROJECT - Lan Hoang and Monica Cruz"
output:
  html_document:
    df_print: paged
---


```{r}
library(caret)
library(dummies)
library(leaps)
library(ggplot2)
library(forecast)
library(gplots)
library(FNN)
library(rpart)
library(rpart.plot)
library(tidyverse)

library(dplyr)
library(lubridate)
library (plotly)

#install.packages("fable")
library(tsibble)
library(fable)
```
 Reading datasets

```{r}

rm(list = ls()) # clean environment
cat("\014") # clean console
setwd("C:\\Users\\lanhh\\OneDrive\\Documents\\UNIGE Statistics\\Masters of Biz Analytics\\Creating Value through Data Mining\\Fall 2021\\5. Final Project")
```


```{r}
sales_train.df <- read.csv("sales_train.csv", header = T, sep = ",", na.strings=c("")) #importing data

sales_test.df <- read.csv("test.csv", header = T, sep = ",", na.strings=c(""))
```

```{r}
items.df <- read.csv("items.csv", header = T, sep = ",", na.strings=c(""))

item_categories.df <- read.csv("item_categories.csv", header = T, sep = ",", na.strings=c(""))

shops.df <- read.csv("shops.csv", header = T, sep = ",", na.strings=c(""))

```

Since the data set might have been designed for Python with the index starting from 0, we adjust the index to start from 1 in this test set to work with R.

```{r}
#sales_test1.df <- sales_test.df %>%
#  mutate(ID = row_number())
#perhaps need to join the data first before we update the ID.
```

```{r}
new_sales <- merge(sales_train.df, items.df, by = "item_id", all.x = TRUE)
#merge_item_shop <- merge(merge_item, shops.df, by = "shop_id", all.x = TRUE)

#merge_item_shop_itemcat <- merge(merge_item_shop, item_categories.df, by = "item_category_id", all.x = TRUE)
```

```{r}
#drop the columns with item_id, shop_id and item_category_id

#drops <- c("item_id","shop_id", "item_category_id")
#sales_train_merged <- merge_item_shop_itemcat[ , !(names(merge_item_shop_itemcat) %in% drops)]
```


```{r}
summary(new_sales)
```
No N/A or missing values in the sales_train set, which is a good thing


```{r}
#convert the date column from string to date format
new_sales$date <- as.Date(new_sales$date, "%d.%m.%Y")
```


Finding the min and max date of the dataset

```{r}
min(new_sales$date)
max(new_sales$date)
summary(new_sales)
```
Now moving onto data cleaning. We notice that there are item prices that are less 
than 0, we need to remove that as it's not logical. 

```{r}
new_sales<- new_sales[new_sales$item_price > 0, ]
new_sales<- new_sales[new_sales$item_cnt_day > 0, ]

```

Data pre-processing

```{r}
#dropping some less useful features like item_name
new_sales$item_name <- NULL

```

```{r}
#reorder the data set by the order of date
new_sales <- new_sales %>% arrange(date, item_id)
summary(new_sales)
```

Aggregate data into company level

```{r}
options(scipen = 999)

#converting daily data into monthly for the item counts (how many items sold per month)
set.seed(1)
new_sales$month <- floor_date(new_sales$date, "month")

monthly_sales <- new_sales%>%
                  group_by(month) %>%
                  summarize(item_cnt_month = sum(item_cnt_day))

monthly_sales_mean<-new_sales%>%
                 group_by(month) %>%
                  summarize(mean_item_cnt_month = mean(item_cnt_day))

```


```{r}
summary(monthly_sales)
```

```{r}
#boxplot of item_cnt_month
boxplot(monthly_sales$item_cnt_month,
        main = "Monthly sales of all stores",
xlab = "Monthly sales (items)",
ylab = "",
col = "orange",
border = "brown",
horizontal = TRUE)

```

```{r}
#create time series object using ts()
monthly_sales.ts <- ts(monthly_sales$item_cnt_month, start = c(2013, 1), end = c(2015, 10), freq = 12)

```

Now we fit a linear regression model to the time series
```{r}
#monthly_sales.lm <- tslm(monthly_sales.ts ~trend + I(trend^2))
```

Overlay the fitted value of the linear model

```{r}
#plot the series
plot(monthly_sales.ts, xlab = "Time", ylab = "Sales Volume by month", ylim = c(63000, 190000))
#lines(monthly_sales.lm$fitted.values, lwd = 2)
```
Overall, the sales trend seems to be decreasing with what seems like seasonal spikes
towards the end of 2013 and end of 2014, perhaps around the holiday season.

Data partitioning: We partition a time series into 2 periods, the earlier period 
is set as the training data and the later period is set as the validation data.
We then apply methods to the earlier training period, and their predictive performance
assessed on the later validation period. 

The base model for benchmark performance - the Naive forecast. This is the most recent
value of the series. Given that this time series has seasonality, we can generate also
a seasonable naive forecast. 

```{r}
#data partition

nValid <- 8
nTrain <- length(monthly_sales.ts) - nValid

train.ts <- window(monthly_sales.ts, start = c(2013, 1), end = c(2013, nTrain))
valid.ts <- window(monthly_sales.ts, start = c(2013, nTrain+1), end = c(2013, nTrain + nValid))

```

Generate naive and seasonal naive forecasts
```{r}
naive.pred <- naive(train.ts, h = nValid)
snaive.pred <- snaive(train.ts, h = nValid)

```

Plot forecasts and actuals in the training and validation sets - ***STILL NEED TO 
GET THE PLOTS TO SHOW CORRECT***
```{r}
plot(train.ts, ylim = c(63000, 290000), ylab = "Sales volume by month", xlab = "Time", bty = "l", 
     xaxt = "n", xlim = c(2013, 2015.9), main = "")
axis(1, at = seq(2013, 2015.9, 1), labels = format(seq(2013, 2015.9, 1)))
lines(naive.pred$mean, lwd = 2, col = "blue", lty = 1)
lines(snaive.pred$mean, lwd = 2, col = "blue", lty = 1)
lines(valid.ts, col = "grey20", lty = 3)
lines(c(2015.8 - 2, 2013.8 - 3), c(0, 20000))
lines(c(2015.8, 2015.8), c(0, 20000))
text(2013, 2015, "Training")
text(2015, 2015.8, "Validation")
text(2015.8, 2016, "Future")
arrows(2015 -2, 18000, 2013.25, 18000, code = 3, length = 0.1, lwd = 1,angle = 30)
arrows(2015.5 - 2, 18000, 2015.3, 18000, code = 3, length = 0.1, lwd = 1,angle = 30)
arrows(2015.5, 18000, 2006, 18000, code = 3, length = 0.1, lwd = 1, angle = 30)

```

Predictive accuracy  of naive and seasonal naive forecast in the validation set 
```{r}
accuracy(naive.pred, valid.ts)

accuracy(snaive.pred, valid.ts)

```
Because there is some element of seasonality in the data set, we see that the
naive method has lower RMSE and is the better model when it comes to the validation set, 
especially when the accuracy on the validation set that will be more indicative
of how the model will perform in the future. This also indicates that the seasonal
naive model overfits the data set. 


REGRESSION-BASED FORECASTING

Based on the time series graph above, we see that this is a time series with a downward
trend while also have seasonality. Thus we will apply gradually models that capture 
trend, seasonality and both by including predictors of both types.

1.A model with only trend

To be able to determine if the time series's trend is additive or multiplicative, 
the time series has to be split into its components.

Extracting the trend

```{r}
monthly_sales_decompose <- decompose(monthly_sales.ts)

monthly_sales_decompose
```

```{r}
#plot the components
plot(monthly_sales_decompose)
```
Since the time series is additive and not multiplicative, we go with producing
a linear trend model

```{r}
#produce linear trend model
train.lm.trend <- tslm(train.ts ~ trend)

#plot the series with trend line
plot(train.ts, xlab ="Time", ylab = "Sales volume by month", ylim = c(63000, 190000))
lines(train.lm.trend$fitted, lwd = 2)
```

2. A model with only seasonality

```{r}
#create a new "Season" column for the data then turn it into dummies (s = 12 seasons so we create 11 dummies)

train.lm.season <- tslm(train.ts ~ season)
summary(train.lm.season)
```
The coefficient for season 12 (56,232) suggests that the sales in December is higher
by 56,232 dollars than that of January each year on average. 

3. A model with both trend and seasonality. 

We now fit a model to the training data with 13 predictors, 11 dummies for month 
and t for trend. 

```{r}
train.lm.trend.season <- tslm(train.ts ~ trend + season)
summary(train.lm.trend.season)
```
Generate trend, seasonal, as well as both trend and seasonal forecasting

```{r}
trend.pred <- forecast(train.lm.trend, h = nValid, level = 0)
season.pred <- forecast(train.lm.season, h = nValid, level = 0)
trend.season.pred <- forecast(train.lm.trend.season, h = nValid)

```


```{r}
accuracy(trend.pred, valid.ts)
accuracy(season.pred, valid.ts)
accuracy(trend.season.pred, valid.ts)
```

Linear regression model with both season and trend seems to perform better than the model with
just season or trend alone, due to lower RMSE in the validation set.
However this is still less accurate than the naive model. 

Next we will try to work on cross-validation for the regression-based model with 
both trend and season.

```{r}
monthly_sales_tsibble <- tsibble::as_tsibble(monthly_sales.ts, index = "month", key = "item_cnt_month")
```


Using stretch_tsibble to create many training sets. We start with a training set 
of length .init = 5 and increasing the size of successive training sets by .step = 1

```{r}
#install.packages("fpp3")
#library(fpp3)
monthly_sales_train <- monthly_sales_tsibble %>%
  select(value) %>%
  stretch_tsibble(.init = 5, .step=1)
monthly_sales_train
```
Fit a regression model to each fold

```{r}
fit_trend <- monthly_sales_train %>%
  model(TSLM(value ~ trend())) %>%
  forecast(h = "1 month") %>%
  group_by(.id) %>%
  mutate(h = row_number()) %>%
  ungroup()

fit_season <- monthly_sales_train %>%
  model(TSLM(value ~ season())) %>%
  forecast(h = "1 month") %>%
  group_by(.id) %>%
  mutate(h = row_number()) %>%
  ungroup()

fit_trend_season <- monthly_sales_train %>%
  model(TSLM(value ~ trend() + season())) %>%
  forecast(h = "1 month") %>%
  group_by(.id) %>%
  mutate(h = row_number()) %>%
  ungroup()
```

Finally, we compare the forecasts against the actual values and average over the folds

```{r}
fit_trend %>% accuracy(monthly_sales_tsibble, by = c("h",".model")) %>%
  select(h, RMSE)

fit_season %>% accuracy(monthly_sales_tsibble, by = c("h",".model")) %>%
  select(h, RMSE)

fit_trend_season %>% accuracy(monthly_sales_tsibble, by = c("h",".model")) %>%
  select(h, RMSE)
```
Even after cross-validation, RMSE of the regression-based model with both trend and season still out-performs models with only either trend or season. However, the naive model still has the lowest RMSE and the strongest predictive performance so far.


```{r}
fit_AR <- monthly_sales_train %>%
  model(ARIMA(value ~ 1 + season())) %>%
  
report(fit_AR)
  
```

Finally, we compare the forecasts against the actual values and average over the folds

```{r}
fc_AR <- forecast(fit_AR, new_data = monthly_sales_tsibble) %>%
  group_by(.id) %>%
  mutate(h= row_number()) %>%
  ungroup()
```
```{r}
fit_AR %>% 
  forecast() %>%
  autoplot(monthly_sales_tsibble)
```

https://fable.tidyverts.org/reference/AR.html
https://robjhyndman.com/hyndsight/tscv-fable/
https://fable.tidyverts.org/reference/


1. Compute and evaluate model with autocorrelation

Autocorrelation
In order to compute the autocorrelation is needed to compute a lagged version of the series.

```{r}
library(TSA)
```

```{r}
#Autocorrelation chart for lags 1 to 12
monthly.sales.ts <- window(train.ts, start= c(2013,1), end=c(2015,10))
acf(monthly.sales.ts, lag.max=12, main="")
```
The graph shows a strong positive correlation during every 12 months, and a slightly 
significant negative correlation in the middle of the year. This reflects a 
seasonality (annual rise at the beginning and the end of the year, as well as a 
drop in the middle of the year).

```{r}
#Examining autocorrelation of the residual series model 
acf(resid(train.lm.trend))
acf(resid(train.lm.season))
acf(resid(train.lm.trend.season))
```
The autocorrelation of residuals graph shows us that in the trend only model,
the yearly model did not capture the seasonal patterns well, neither did the
season-only model. Meanwhile, trend+season regression model captured both the trend
and season adequately.

We notice that in the regression model with season, there is a strong positive
correlation from lag 1 on, indicating a positive relationship between neighboring 
residuals. As this is valuable information, we can use this to improve the forecast
of this model below by building the autocorrelation into the regression model.

The first regression-type model is AR (autoregression) model. AR(1) because we
use a linear model fit on an additive series. 

```{r}

train.res.arima<-Arima(train.lm.season$residuals,order=c(1,0,0))
valid.res.arima.pred<-forecast(train.res.arima, h=1)

summary(train.res.arima)
valid.res.arima.pred
```
The AR(1) coefficient (0.837) is close to the lag-1 correlation that we found 
earlier. 

```{r}
accuracy(valid.res.arima.pred, valid.ts)
```

We look at the RMSE of the validation set to evaluate predictive performance and 
see that the error rate is relatively high. This shows us once again that the
model does not fit properly the forecasting, therefore the naive method continue 
to be the ideal.
=======

2. Try out KNN regression for time series as another model

```{r}
#Resources used for this section and could be included in the final report:
#https://cran.r-project.org/web/packages/tsfknn/vignettes/tsfknn.html
#https://rpubs.com/davoodastaraky/TSA1
#https://www.r-bloggers.com/2020/09/time-series-forecasting-knn-vs-arima/
#
```


```{r}
#install.packages("tsfknn")
library(tsfknn)
```
A couple of parameters that we had to consider: 
h = 1: the forecast horizon, that is, the number of future values to be predicted is 1
for November 2015

lags : because our time series data is monthly, we set the parameters to 1:12. 
The last 12 observations of the data build the instance, which is shown by purple points on the graph.

k : the number of nearest neighbors used by the KNN model. In order to specify a KNN model, we have to select the value of the k parameter. Several strategies can be used to choose this value. A first, fast, straightforward solution is to use some heuristic (it is recommended setting k to the square root of the number of training examples). Other approach is to select k using an optimization tool on a validation set. k should minimize a forecast accuracy measure. The optimization strategy is very time consuming.

A third strategy which we are using is to use several KNN models with different k values. 
Each KNN model generates its forecasts and the forecasts of the different models are 
averaged to produce the final forecast. This strategy is based on the success of model 
combination in time series forecasting. This way, the use of a time consuming optimization 
tool is avoided and the forecasts are not based on an unique, heuristic k value. 
We can use of this strategy specifying a vector of k values, in this case from 3 to 12.

KNN has a general reputation as not suitable for forecasting a time series with a trend. 
The reason is that KNN predicts an average of historical values of the time series, 
so it cannot predict correctly values out of the range of the time series. 
As our time series has a trend, we use the parameter transform to transform the 
training samples. We need to determining whether to use the value "additive" for 
if the trend is additive or "multiplicative" for multiplicative time series.

The data has been determined as additive above.


Given that the time series is additive, we can now run the forecast 
```{r}
knn_pred <- knn_forecasting(monthly_sales.ts, h = 1, lags = 1:12, k = c(3,12), transform = "additive")


plot(knn_pred)
```



```{r}
#KNN Accuracy withou cross validation

ro1 <- rolling_origin(knn_pred, h = 9,rolling = FALSE)

#KNN Accuracy using cross validation
ro <- rolling_origin(knn_pred, h = 9,rolling = TRUE)

print(ro$test_sets)
```
Every row of the matrix contains a different test set. The first row is a test set with the last h values of the time series, the second row a test set with the last h - 1 values of the time series and so on. Each test set has an associated training test with all the data in the time series not belonging to the test set. For every training set a KNN model with the parameters associated with the original model is built and the test set is predicted. We can see the predictions as follows:

```{r}
print(ro$predictions)
```
and also the errors in the predictions:

```{r}
print(ro$errors)
```


```{r}
#Several forecasting accuracy measures applied to all the errors in the different test sets can be consulted:
ro$global_accu
ro1$global_accu

#We now consult the forecasting accuracy measures for every forecasting horizon:
ro$h_accu

```
Based on the RMSE, the model's performance is comparable to that of the regression model, 
it performs better than the seasonal naive model but but still slightly less 
accurate than the naive model.

```{r}
#Finally, a plot with the predictions for a given forecast horizon can be generated:

plot(ro, h = 1)
```

```{r}
#NEED TO CREATE A SUMMARY TABLE WITH ALL RMSE OF ALL MODELS IN ONE PLACE
Accuracy.Table <- data.frame("Method" = c("Naive",
                                  "Seasonal Naive",
                                  "Regression-based with Trend",
                                  "Regression-based with Season",
                                  "Regression-based with Trend and Season",
                                  "Regression-based with Trend - CV",
                                  "Regression-based with Season - CV",
                                  "Regression-based with Trend and Season - CV",
                                  "Regression-based with Season + Autocorrelation ",
                                  "KNN",
                                  "KNN - CV "),
                              
                               "RMSE" = c(accuracy(naive.pred, valid.ts)[4],
                                              accuracy(snaive.pred, valid.ts)[4],
                                              accuracy(trend.pred, valid.ts)[4],
                                              accuracy(season.pred, valid.ts)[4],
                                              accuracy(trend.season.pred, valid.ts)[4],
                                              24557.37,
                                              28918.52,	
                                              19884.95,
                                            accuracy(valid.res.arima.pred, valid.ts)[4],
                                          ro1$global_accu[1],  
                                          ro$global_accu[1])
                                                 )

Accuracy.Table

```

 
